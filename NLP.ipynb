{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just setting up the imports and the basic sentence/paragraph that is going to be used\n",
    "throughout the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'original:', 566, u'The')\n",
      "(u'lowercased:', 501, u'the')\n",
      "(u'lemma:', 501, u'the')\n",
      "(u'shape:', 567, u'Xxx')\n",
      "(u'prefix:', 568, u'T')\n",
      "(u'suffix:', 566, u'The')\n",
      "(u'log probability:', -5.774222373962402)\n",
      "(u'Brown cluster id:', 30)\n",
      "----------------------------------------\n",
      "(u'original:', 2703, u'quick')\n",
      "(u'lowercased:', 2703, u'quick')\n",
      "(u'lemma:', 2703, u'quick')\n",
      "(u'shape:', 515, u'xxxx')\n",
      "(u'prefix:', 994, u'q')\n",
      "(u'suffix:', 1806, u'ick')\n",
      "(u'log probability:', -9.907200813293457)\n",
      "(u'Brown cluster id:', 295)\n",
      "----------------------------------------\n",
      "(u'original:', 4883, u'brown')\n",
      "(u'lowercased:', 4883, u'brown')\n",
      "(u'lemma:', 4883, u'brown')\n",
      "(u'shape:', 515, u'xxxx')\n",
      "(u'prefix:', 537, u'b')\n",
      "(u'suffix:', 766, u'own')\n",
      "(u'log probability:', -10.879706382751465)\n",
      "(u'Brown cluster id:', 215)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up spaCy\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "from nltk.corpus import verbnet as vn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.en import English\n",
    "from __future__ import unicode_literals\n",
    "from spacy.symbols import nsubj,VERB\n",
    "parser = English()\n",
    "\n",
    "\n",
    "# Test Data\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "                 \n",
    " \n",
    "# Remember that this has to be in unicode in order to be used correctly\n",
    "\n",
    "\n",
    "# all you have to do to parse text is this:\n",
    "#note: the first time you run spaCy in a file it takes a little while to load up its modules\n",
    "document = parser(sentence)\n",
    "\n",
    "# Let's look at the tokens\n",
    "# All you have to do is iterate through the parsedData\n",
    "# Each token is an object with lots of different properties\n",
    "# A property with an underscore at the end returns the string representation\n",
    "# while a property without the underscore returns an index (int) into spaCy's vocabulary\n",
    "# The probability estimate is based on counts from a 3 billion word\n",
    "# corpus, smoothed using the Simple Good-Turing method.\n",
    "for i, token in enumerate(document):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    if i > 1:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'The', u'det', u'jumps', [], [])\n",
      "(u'quick', u'amod', u'jumps', [], [])\n",
      "(u'brown', u'amod', u'jumps', [], [])\n",
      "(u'fox', u'nsubj', u'jumps', [], [])\n",
      "(u'jumps', u'ROOT', u'jumps', [u'The', u'quick', u'brown', u'fox'], [u'over', u'.'])\n",
      "(u'over', u'prep', u'jumps', [], [u'dog'])\n",
      "(u'the', u'det', u'dog', [], [])\n",
      "(u'lazy', u'amod', u'dog', [], [])\n",
      "(u'dog', u'pobj', u'over', [u'the', u'lazy'], [])\n",
      "(u'.', u'punct', u'jumps', [], [])\n"
     ]
    }
   ],
   "source": [
    "# shown as: original token, dependency tag, head word, left dependents, right dependents\n",
    "for token in document:\n",
    "    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'The', u'(not an entity)')\n",
      "(u'quick', u'(not an entity)')\n",
      "(u'brown', u'(not an entity)')\n",
      "(u'fox', u'(not an entity)')\n",
      "(u'jumps', u'(not an entity)')\n",
      "(u'over', u'(not an entity)')\n",
      "(u'the', u'(not an entity)')\n",
      "(u'lazy', u'(not an entity)')\n",
      "(u'dog', u'(not an entity)')\n",
      "(u'.', u'(not an entity)')\n",
      "-------------- entities only ---------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for token in document:\n",
    "    print(token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\")\n",
    "\n",
    "print(\"-------------- entities only ---------------\")\n",
    "ents = list(document.ents)\n",
    "for entity in ents:\n",
    "    print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for Determining the Verb Context portion of the Jupyter notebook\n",
    "The easiest way to determine what hte context of a verb is, is by first finding \n",
    "what the subject of the sentence is. A verb alone doesn't tell you the context\n",
    "but its relationship to the subject often gives a better idea. So first we must\n",
    "define what the subject of the sentence is, then we can look for patterns and \n",
    "the relationship between the verb and the subject as well as the other words in\n",
    "the sentence. This is a tree that shows a simple heirarchy between the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                jumps                  \n",
      "  ________________|____________         \n",
      " |    |     |     |    |      over     \n",
      " |    |     |     |    |       |        \n",
      " |    |     |     |    |      dog      \n",
      " |    |     |     |    |    ___|____    \n",
      "The quick brown  fox   .  the      lazy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in document.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tree we can somewhat see the relationship between all the words and how they\n",
    "are interconnected. This isn't perfect though because we still don't necessarily know\n",
    "the parts of speech associated with each of the words that have been mapped out. So \n",
    "now we are going to look at the relationship by using a few libraries that have built\n",
    "in methods to parse the words, then develop their dependecies. First we will look at\n",
    "the parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: POS\n",
      "--------\n",
      "The: DET\n",
      "quick: ADJ\n",
      "brown: ADJ\n",
      "fox: NOUN\n",
      "jumps: VERB\n",
      "over: ADP\n",
      "the: DET\n",
      "lazy: ADJ\n",
      "dog: NOUN\n",
      ".: PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(\"WORD: POS\\n--------\")\n",
    "for w in document:\n",
    "    print\"%s: %s\" % (w,w.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the representation of the parts of speech that are within the sentence. This can give us a clue as to how to build common patterns within the sentence, so that we can better understand what common sentence structures are and the context given 'X' pattern. Now we can use spaCy to look into the relationship between the verbs and the words around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: DEP\n",
      "--------\n",
      "The: det\n",
      "quick: amod\n",
      "brown: amod\n",
      "fox: nsubj\n",
      "jumps: ROOT\n",
      "over: prep\n",
      "the: det\n",
      "lazy: amod\n",
      "dog: pobj\n",
      ".: punct\n"
     ]
    }
   ],
   "source": [
    "print(\"WORD: DEP\\n--------\")\n",
    "sentence = next(document.sents) \n",
    "for word in sentence:\n",
    "    print \"%s: %s\" % (word,word.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have both the parts of speech as well as the dependencies, we can now look specifically at the interactions between the verbs and the words around them. We can pull out the words that are designated as verbs, and make an n-gram out of them. From this we can look for synonyms that would fit well in that position of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'bring-11.3', u'characterize-29.2', u'convert-26.6.2', u'cost-54.2', u'fit-54.3', u'performance-26.7-2', u'steal-10.5']\n",
      "\n",
      "{u'convert': u'26.6.2', u'fit': u'54.3', u'characterize': u'29.2', u'bring': u'11.3', u'cost': u'54.2', u'performance': u'26.7', u'steal': u'10.5'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sent = [w for w in document if not w in stop_words]\n",
    "filtered_sent = []\n",
    "\n",
    "# pulling out the stop words from the\n",
    "# tokenized sentence. Adding them to\n",
    "# list of filtered words.\n",
    "for w in document:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "        \n",
    "# Replace 'take' with verb in sentence.        \n",
    "word = vn.classids('take')\n",
    "print(word)\n",
    "\n",
    "# Splitting the classes from the word\n",
    "name, theme = zip(*(s.split(\"-\") for s in word))\n",
    "dictionary = dict(zip(name, theme))\n",
    "\n",
    "print\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the synonyms and their respective identifier with eachother. This makes it easier to reference and loop though to find the themes within the verbs."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
